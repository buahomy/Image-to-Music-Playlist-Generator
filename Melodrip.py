# -*- coding: utf-8 -*-
"""Melodrip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FV_B7by20IdGuBS1goGFBVbOFCEqbz_E
"""

#implementation from https://huggingface.co/docs/transformers/main/model_doc/blip-2
import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

img_url = 'https://images.pexels.com/photos/1629781/pexels-photo-1629781.jpeg?auto=compress&cs=tinysrgb&w=800'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

contains_person = True

# Conditional image captioning
text = "a photography of"
inputs = processor(raw_image, text, return_tensors="pt")
out = model.generate(**inputs)
conditional_caption = processor.decode(out[0], skip_special_tokens=True)

# Unconditional image captioning
inputs = processor(raw_image, return_tensors="pt")
out = model.generate(**inputs)
unconditional_caption = processor.decode(out[0], skip_special_tokens=True)

# Store in variables
conditional_caption_variable = processor.decode(out[0], skip_special_tokens=True)
unconditional_caption_variable = processor.decode(out[0], skip_special_tokens=True)

# Because we don't know which caption to print
if contains_person:
    text_caption = conditional_caption_variable
else:
    text_caption = unconditional_caption_variable

print("Input:", text_caption)

#initializing functions used for extracting keywords
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import networkx as nx

nltk.download('punkt_tab')
nltk.download('stopwords')

def preprocessing(text):
    #tokenizing the text (turned them into sentences, then words)
    sentences = sent_tokenize(text)
    words = [word_tokenize(sentence.lower()) for sentence in sentences]

    #removing stop words and punctuation
    stop_words = set(stopwords.words('english'))
    filtered_words = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in words]

    #turn words into the stem word
    stemmer = PorterStemmer()
    stemmed_words = [[stemmer.stem(word) for word in sentence] for sentence in filtered_words]

    return stemmed_words

#creates a graph model of the most notable words in the sentence
def buildGraph(words):
    graph = nx.Graph()

    #nodes of the graph
    for sentence in words:
        for word in sentence:
            if not graph.has_node(word):
                graph.add_node(word)

    #edges of the graph
    for sentence in words:
        for i, word1 in enumerate(sentence):
            for j, word2 in enumerate(sentence):
                if i != j:
                    if not graph.has_edge(word1, word2):
                        graph.add_edge(word1, word2)

    return graph

#ranks the tokenized text using the graph
def textRank(graph, num_iterations=100, d=0.85):
    scores = {node: 1.0 for node in graph.nodes()}

    for i in range(num_iterations):
        next_scores = {}
        for node in graph.nodes():
            score = 1 - d
            for neighbor in graph.neighbors(node):
                score += d * (scores[neighbor] / len(list(graph.neighbors(neighbor))))
            next_scores[node] = score
        scores = next_scores

    return scores

def extractKeywords(text, num_keywords=5):
  """extracts the keywords of a given text"""
  words = preprocessing(text)
  graph = buildGraph(words)
  scores = textRank(graph)

  #Sort the words by highest to lowest textRank score
  ranked_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)

  #Extract the top [num_keywords]keywords(default of 5)
  keywords = [word for word, score in ranked_words[:num_keywords]]

  return keywords

#the final function in action
keywords = extractKeywords(text_caption)
print("Keywords:", keywords)

#visualizing what is happening above
import networkx as nx
import matplotlib.pyplot as plt

# Visualizes the graph
def visualizeGraph(graph):
    pos = nx.spring_layout(graph)  # positions for all nodes
    # nodes
    nx.draw_networkx_nodes(graph, pos, node_size=700)
    # edges
    nx.draw_networkx_edges(graph, pos, width=1.0, alpha=0.5)
    # labels
    nx.draw_networkx_labels(graph, pos, font_size=10, font_family="sans-serif")
    plt.axis("off")
    plt.show()

# Visualizes the TextRank scores
def visualizeScores(scores):
    plt.bar(scores.keys(), scores.values(), align="center", alpha=0.5)
    plt.xlabel("Words")
    plt.ylabel("TextRank Score")
    plt.title("TextRank Scores")
    plt.xticks(rotation=90)
    plt.show()

# visualizing the example
words = preprocessing(text_caption)
theGraph = buildGraph(words)
visualizeGraph(theGraph)
visualizeScores(textRank(theGraph))

#song recommendation code
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

#dataset
songs = pd.read_csv('https://drive.google.com/file/d/1vf838bwv1pxXBgrQxojUGvUJwzWCGsCZ/view?usp=drive_link')

#preprocessing the dataset
songs.drop("link", axis=1, inplace=True)
songs['text'] = songs['text'].str.replace(r'\n', '')

#TF-IDF vectorization
tfidf = TfidfVectorizer(stop_words='english')
lyrics_matrix = tfidf.fit_transform(songs['text'])

#matching keywords to each song
def songRecommender(keywords):
  recommendations = {}
  for keyword in keywords:
    keyword_tfidf = tfidf.transform([keyword])
    #finds the similarity between the keywords
    keyword_similarity = cosine_similarity(keyword_tfidf, lyrics_matrix)
    for i, song in enumerate(songs['text']):
        if song not in recommendations:
            recommendations[song] = keyword_similarity[0][i]
        else:
            recommendations[song] += keyword_similarity[0][i]
  #rank song based on the aggregated similarity scores
  ranked_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
  return ranked_recommendations

#the code being used with the keywords depicted in the codeblock above
recommendations = songRecommender(keywords)

num_recommendations = 5 #global variable that will be used to extract the top 5

#function that shows why the song was chosen
for song, similarity_score in recommendations[:num_recommendations]:
    print("Lyrics:", song)
    print("Similarity Score:", similarity_score)
    print()  #make the lyrics readable

def printTopFive(recommendations, num_recommendations):
    top_five = []
    for song, similarity_score in recommendations[:num_recommendations]:
        song_info = songs[songs['text'] == song].iloc[0]
        song_name = song_info['song']
        artist = song_info['artist']
        top_five.append((song_name, artist, similarity_score))
    return top_five if top_five else []

#function in action, using the examples that has been explored so far
printTopFive(recommendations, num_recommendations) #will also run with songRecommender(keywords) in place of recommendations

#function to visualize the top recommendations (used in the report)
def visualizeTopRecommendations(topRecs):
    if not topRecs:
        print("No recommendations to visualize.")
        return

    # Extract song names, artists, and similarity scores
    song_names = [recommendation[0] for recommendation in topRecs]
    artists = [recommendation[1] for recommendation in topRecs]
    similarity_scores = [recommendation[2] for recommendation in topRecs]

    # Create bar graph
    plt.figure(figsize=(10, 6))
    plt.barh(song_names, similarity_scores, color='skyblue')
    plt.xlabel('Similarity Score')
    plt.ylabel('Songs')
    plt.title('Top Recommendations with Similarity Scores')
    plt.gca().invert_yaxis()  # Invert y-axis to display the highest similarity at the top
    plt.show()

topRecs = printTopFive(recommendations, 10)
visualizeTopRecommendations(topRecs)

# Interface
import streamlit as st

# Streamlit app
def main():
    st.title("Playlist Recommendation from Image Captioning")

    image_url = st.text_input("Paste the URL link of the image:")
    if st.button("Generate Playlist"):
        # need to write the image captioning again for the interface since the first code was used for demonstration how this works
        if image_url:
            raw_image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')
            contains_person = True
            text = "a photography of"

            # Conditional image captioning
            inputs = processor(raw_image, text, return_tensors="pt")
            out = model.generate(**inputs)
            conditional_caption_variable = processor.decode(out[0], skip_special_tokens=True)

            # Unconditional image captioning
            inputs = processor(raw_image, return_tensors="pt")
            out = model.generate(**inputs)
            unconditional_caption_variable = processor.decode(out[0], skip_special_tokens=True)

            if contains_person:
                text_caption = conditional_caption_variable
            else:
                text_caption = unconditional_caption_variable

            st.write("Input Image Caption:", text_caption)

            # keywords extraction from image caption above
            keywords = extractKeywords(text_caption)

            # Recommend songs based on keywords
            recommended_songs = songRecommender(keywords)

            # Display top5 recommendations
            top_five_songs = printTopFive(recommended_songs, 5)
            st.write("Top 5 Recommended Songs:")
            for idx, (song_name, artist, similarity_score) in enumerate(top_five_songs):
                st.write(f"{idx+1}. {song_name} by {artist} (Similarity Score: {similarity_score:.2f})")

if __name__ == "__main__":
    main()
